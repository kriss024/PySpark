{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext(master='local[*]', appName='Spark_App')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0654dcdc35b2:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark_App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark_App>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark UI http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wzorzec map-reduce to sposób takiego organizowania przetwarzania, aby wykorzystać potencjał wielu maszyn w klastrze i jednocześnie zgrupować możliwie jak najwięcej przetwarzania i przetwarzanych danych w tych samych miejscach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utworzenie nowego RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Główna abstrakcja Spark zapewnia odporny rozproszony zestaw danych RDD (Resilient Distributed Datasets), który jest zbiorem elementów podzielonych między węzły klastra, na których można operować równolegle.\n",
    "\n",
    "Kolekcja rozproszona:\n",
    "RDD wykorzystuje operacje MapReduce, które są powszechnie stosowane do przetwarzania i generowania dużych zestawów danych za pomocą równoległego, rozproszonego algorytmu w klastrze. Umożliwia użytkownikom pisanie obliczeń równoległych przy użyciu zestawu operatorów wysokiego poziomu, bez martwienia się o rozkład pracy i odporność na uszkodzenia.\n",
    "\n",
    "Niezmienne: RDD złożone ze zbioru rekordów podzielonych na partycje. Partycja jest podstawową jednostką równoległości w RDD, a każda partycja jest jednym logicznym podziałem danych, który jest niezmienny i utworzony przez pewne transformacje istniejących partycji. Niezmienność pomaga osiągnąć spójność obliczeń.\n",
    "\n",
    "Odporny na błędy: W przypadku utraty części partycji RDD, możemy odtworzyć transformację na tej partycji w linii, aby uzyskać to samo obliczenie, a nie przeprowadzanie replikacji danych w wielu węzłach. Ta cecha jest największą zaletą RDD, ponieważ oszczędza wiele wysiłku w zarządzaniu danymi i replikacji, a tym samym zapewnia szybsze obliczenia.\n",
    "\n",
    "Leniwe oceny: Wszystkie transformacje w Spark są leniwe, ponieważ nie obliczają od razu swoich wyników. Zamiast tego pamiętają po prostu transformacje zastosowane do jakiegoś podstawowego zestawu danych. Transformacje są obliczane tylko wtedy, gdy akcja wymaga zwrócenia wyniku do programu sterownika.\n",
    "\n",
    "Transformacje funkcjonalne: RDD obsługują dwa typy operacji: transformacje, które tworzą nowy zestaw danych z istniejącego, oraz akcje, które zwracają wartość do sterownika program po uruchomieniu obliczeń na zbiorze danych.\n",
    "\n",
    "Formaty przetwarzania danych:\n",
    "Może łatwo i wydajnie przetwarzać dane zarówno ustrukturyzowane, jak i nieustrukturyzowane.\n",
    "\n",
    "Obsługiwane języki programowania:\n",
    "Interfejs API RDD jest dostępny w Javie, Scali, Python i R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMACJE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5])\n",
    "squared = nums.map(lambda x: x * x)\n",
    "results = squared.collect()\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,2,3,4,4,5])\n",
    "distinct = nums.distinct()\n",
    "print(distinct.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 4, 5, 6, 7]\n",
      "[4, 5]\n",
      "[1, 2, 3]\n",
      "[(1, 4), (1, 5), (2, 4), (2, 5), (1, 6), (1, 7), (2, 6), (2, 7), (3, 4), (3, 5), (4, 4), (4, 5), (5, 4), (5, 5), (3, 6), (3, 7), (4, 6), (4, 7), (5, 6), (5, 7)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "rdd2 = sc.parallelize([4,5,6,7])\n",
    "\n",
    "union = rdd1.union(rdd2)\n",
    "print(union.collect())\n",
    "\n",
    "intersection = rdd1.intersection(rdd2)\n",
    "print(intersection.collect())\n",
    "\n",
    "subtract = rdd1.subtract(rdd2)\n",
    "print(subtract.collect())\n",
    "\n",
    "cartesian = rdd1.cartesian(rdd2)\n",
    "print(cartesian.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AKCJE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5])\n",
    "sum = nums.reduce(lambda x, y: x + y)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5])\n",
    "sum = nums.fold(0, lambda x, y: x + y)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1,1,3,3,5])\n",
    "def f(x): print(x)\n",
    "\n",
    "nums.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n = 10\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "inside(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(0,n)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(0,n)).filter(inside).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(0,n)).filter(inside).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "\n",
    "count = sc.parallelize(range(0,n)).filter(inside).count()\n",
    "pi = 4*count/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.143196\n"
     ]
    }
   ],
   "source": [
    "print(\"Pi is roughly %f\" % (pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utworzenie nowego DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame to rozproszony zbiór danych zorganizowany w nazwane kolumny. Jest to koncepcyjnie ekwiwalent tabeli w relacyjnej bazie danych lub ramce danych R/Python. Wraz z Dataframe firma Spark wprowadziła również optymalizator katalizatora, który wykorzystuje zaawansowane funkcje programowania do budowy rozszerzalnego optymalizatora zapytań.\n",
    "\n",
    "Rozproszony zbiór obiektu wiersza: DataFrame to rozproszony zbiór danych zorganizowany w nazwane kolumny. Jest to koncepcyjnie odpowiednik tabeli w relacyjnej bazie danych, ale z bogatszymi optymalizacjami pod maską.\n",
    "\n",
    "Przetwarzanie danych: Przetwarzanie ustrukturyzowanych i nieustrukturyzowanych formatów danych (Avro, CSV, wyszukiwanie elastyczne i Cassandra) oraz systemów pamięci masowej (HDFS, tabele gałęzi, MySQL itp. ). Może czytać i pisać ze wszystkich tych różnych źródeł danych.\n",
    "\n",
    "Strukturyzując dane, DataFrames pozwala silnikowi Apache Spark (Catalyst Optimizer) udoskonalić wydajność zapytań. \n",
    "\n",
    "Zgodność gałęzi: Za pomocą Spark SQL można uruchamiać niezmodyfikowane zapytania Hive w istniejących magazynach Hive. Ponownie wykorzystuje interfejs Hive i interfejs MetaStore i zapewnia pełną zgodność z istniejącymi danymi Hive, zapytaniami i UDF.\n",
    "\n",
    "Wolfram: Wolfram zapewnia fizyczny backend wykonawczy, który jawnie zarządza pamięcią i dynamicznie generuje kod bajtowy do oceny wyrażenia.\n",
    "\n",
    "Obsługiwane języki programowania:\n",
    "Interfejs API Dataframe jest dostępny w Javie, Scali, Python i R.\n",
    "\n",
    "Wykonanie zapytań w Pythonie może być znacząco wolniejsze, z powodu potrzeby komunikacji między Java JVM i Py4J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder\\\n",
    "#        .master(\"local[*]\")\\\n",
    "#        .appName(\"Spark_App\")\\\n",
    "#        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0654dcdc35b2:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark_App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark_App>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "| Amber| 22|\n",
      "|Alfred| 23|\n",
      "|  Skye|  4|\n",
      "|Albert| 12|\n",
      "| Amber|  9|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(\n",
    "    [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), \n",
    "     ('Amber', 9)])\n",
    "\n",
    "df = spark.createDataFrame(rdd).toDF(\"name\", \"age\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,2,3),(4,5,6),(7,8,9)])\n",
    "df = rdd.toDF([\"a\",\"b\",\"c\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak już użyliśmy `.collect()` do utworzenia RDD, możemy dostać się do danych obiektu, tak jak to robimy w Pythonie:\n",
    "`data_heterogenous[1]['Porsche']`, dla którego Spark zwraca `100000`.\n",
    "Metoda `.collect()` zwraca wszystkie elementy RDD do programu „driver”, gdzie jest szeregowany jako lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_heterogenous = sc.parallelize([('Ferrari', 'fast'), {'Porsche': 100000},  ['Spain','visited', 4504]]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(data_heterogenous[1]['Porsche'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.sql('''select 'spark' as hello ''')\n",
    "data.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utworzymy DataFrame przez wygenerowanie danych. W naszym przykładzie utworzymy `stringJSONRDD` RDD, a następnie przekonwertujemy to do formatu DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringJSONRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utworzymy również tabelę tymczasową przez użycie metody `createOrReplaceTempView`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON.createOrReplaceTempView('swimmersJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak już wiemy, wiele operacji RDD to transformacje, które nie są wykonywane dopóki nie jest wykonana operacja „akcja” (action). \n",
    "Przykładowo, w `sc.parallelize` jest transformacją, która jest wykonana, gdy dokonujemy konwersji z RDD do DataFrame używając `spark.read.json` (akcja)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Używając DataFrame API, możemy wywołać metodę `show(<n>)`, która zwraca pierwsze n wierszy na konsolę (domyślnie 10 wierszy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor|id |name   |\n",
      "+---+--------+---+-------+\n",
      "|19 |brown   |123|Katie  |\n",
      "|22 |green   |234|Michael|\n",
      "|23 |blue    |345|Simone |\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak już utworzyliśmy `swimmersJSON` DataFrame, to możemy uruchomić DataFrame API, jak również zapytania SQL na tym DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym przypadku,  określamy schemat przez dostarczenie do Spark SQL typów danych (pyspark.sql.types) i wygenerowanie kilku przykładowych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "stringCSVRDD = sc.parallelize([\n",
    "(123, 'Katie', 19, 'brown'),\n",
    "(234, 'Michael', 22, 'green'),\n",
    "(345, 'Simone', 23, 'blue')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"id\", LongType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"age\", LongType(), True),\n",
    "StructField(\"eyeColor\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zastosowanie schematu do RDD i utworzenie DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utworzenie tymczasowego widoku używają DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby uzyskać liczbę wierszy w DataFrame możemy użyć metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmersJSON.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby uruchomić instrukcję filtrującą możemy użyć klauzulę `filter`. W poniższych poleceniach używamy również klauzuli `select`, aby określić zwracane kolumny:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołajmy teraz te same zapytania, ale używając zapytań SQL w stosunku do tej samej DataFrame. Warto nadmienić, że DataFrame zawierająca dane pływaków jest dostępna z uwagi na wykonane wcześniej polecenie z metodą `.createOrReplaceTempView`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, age from swimmers where age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ważną uwagą, gdy pracujemy z Spark SQL i DataFrames, jest łatwość pracy z popularnymi formatami przechowywania danych *CSV*, *JSON*, zaś powszechnie stosowanym formatem przechowywania danych dla zapytań Spark SQL jest format pliku *Parquet*. \n",
    "Jest to format kolumnowy, który jest wspierany przez wiele innych systemów przetwarzania danych, a Spark SQL wspiera zarówno odczyt i zapis z/do plików *Parquet*, które automatycznie zachowują oryginalny schemat danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\")\n",
    "df.write.parquet(\"mydata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(\"mydata.parquet\")\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
